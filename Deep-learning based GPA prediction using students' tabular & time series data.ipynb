{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb5398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from tensorflow.keras import optimizers, regularizers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Model, Sequential, Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, concatenate, Reshape, Activation, Flatten, Input, LSTM, Add\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7752eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Keras requires the input size for every samples to be the same, however each sample/student\n",
    "   has different number of semesters in the university.We found that the maximum number of semesters\n",
    "   taken by an undergraduate student is up to 16. Therefore, we created a 15-point GPA time series for\n",
    "   each student to predict the last semeseter GPA.\n",
    "   \n",
    "   Since not all students required 16 semesters to finish their study, we filled in missing values \n",
    "   for time points beyond the last semester. For example, if the last recorded data of a student \n",
    "   is in the sixth semester, we obtain the GPA series of the student as follows:\n",
    "   \n",
    "   gpa_series = [2.9, 1.8, 2.8, 4.0, 3.1]\n",
    "   \n",
    "   Notice that We exclude GPA for the sixth semester because it is used for the y-value (predicted last GPA).\n",
    "   \n",
    "   We further padded the above list by filling in some random numerical value (e.g., -1 in our case)\n",
    "   to create a 15-point time series, therefore we have:\n",
    "   \n",
    "   gpa_series = [2.9, 1.8, 2.8, 4.0, 3.1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]''' \n",
    "\n",
    "#load the processed data\n",
    "processedData = pd.read_csv('SASC/data_example.csv')\n",
    "\n",
    "#drop rows with LA status\n",
    "processedData = processedData[(processedData['ProgramStatus']!='LA')] \n",
    "\n",
    "#drop redundant columns\n",
    "drop_columns = [\"smt\", \"leave\", \"n_gpa\", \"ProgramStatus\"]\n",
    "processedData = processedData.drop(drop_columns, axis=1)\n",
    "\n",
    "#encode the following categorical columns\n",
    "encodedData = pd.get_dummies(processedData, columns=['Campus', 'AcademicProgramDescription'])\n",
    "\n",
    "#shift column \"smt_gpa_last\" and \"last_gpa\" to the very right of the table\n",
    "lb = [\"smt_gpa_last\", \"last_gpa\"]\n",
    "encodedData = encodedData[[c for c in encodedData if c not in lb] + [c for c in lb if c in encodedData]]\n",
    "\n",
    "#separate data modality\n",
    "D = encodedData.values\n",
    "Ds = D[:, :16] #sequential data\n",
    "Df = D[:, 16:] #tabular data\n",
    "\n",
    "#obtain the last semester of each student (excluding the semester for last_gpa)\n",
    "last_smt = Df[:,-2].astype(np.int64) - 1\n",
    "\n",
    "#create a list of semesters after semester last_smt\n",
    "idx_last_smt = [[x for x in range(last_smt[i], 16)] for i in range(len(last_smt))]\n",
    "\n",
    "#create a list of GPA\n",
    "gpas=[]\n",
    "for i in range(len(Ds)):\n",
    "    gpa = np.array(Ds[i])\n",
    "    gpa = np.delete(gpa, idx_last_smt[i]).tolist()\n",
    "    gpas.append(gpa)\n",
    "\n",
    "#fill in missing values in the list with zeros\n",
    "gpas = [[0 if x != x else x for x in gpas[i]] for i in range(len(gpas))]\n",
    "\n",
    "#create padded sequences\n",
    "padded_Ds = sequence.pad_sequences(gpas, padding='post',\n",
    "                                   maxlen=15,\n",
    "                                   value=-1.0, \n",
    "                                   dtype='float32')\n",
    "\n",
    "#combine together padded_Ds and Df\n",
    "D_new = [np.concatenate([ds,df]) for ds, df in zip(padded_Ds, Df)]\n",
    "D_new = np.array(D_new)\n",
    "\n",
    "#create data input\n",
    "y = D_new[:,-1]\n",
    "X = D_new[:,:-1]\n",
    "\n",
    "#split train, valid, test set\n",
    "test_size = 0.20\n",
    "seed = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "#separate the sequential data (s) and tabular data(t)\n",
    "Xs_train = X_train[:, :15]\n",
    "Xt_train = X_train[:, 15:]\n",
    "Xs_test = X_test[:, :15]\n",
    "Xt_test = X_test[:, 15:]\n",
    "\n",
    "Xs_train = np.expand_dims(Xs_train, axis=-1)\n",
    "Xs_test = np.expand_dims(Xs_test, axis=-1)\n",
    "\n",
    "#Scale and Normalize the data\n",
    "scaler_train = StandardScaler().fit(Xt_train)\n",
    "Xt_train = scaler_train.transform(Xt_train)\n",
    "scaler_test = StandardScaler().fit(Xt_test)\n",
    "Xt_test = scaler_test.transform(Xt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b8bbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define three models: \n",
    "   1) ann: MLP with several layers for tabular data input only, \n",
    "   2) lstm: LSTM layers for sequential data input only, and \n",
    "   3) lstm_ann: LSTM layers and MLP layers are merged by concenating the last layer of each structure.\n",
    "      The model is for two-headed input.\n",
    "   4) proposed: The proposed model  described in the paper (kind of)\n",
    "   \n",
    "   The parameters used in the following models (num. of layer, neurons, regularizer, etc) are different \n",
    "   with those proposed in the paper because we should have perform hyperparameter tuning first (including\n",
    "   the regularizers that I skipped here). Here we only gave the general idea about the architecture of such models.'''\n",
    "\n",
    "def ann():\n",
    "    t_model = Sequential()\n",
    "    t_model.add(Dense(64, activation=\"relu\", kernel_initializer='uniform', input_dim=Xt_train.shape[1]))\n",
    "    t_model.add(Dropout(0.4))\n",
    "    t_model.add(Dense(256, activation=\"relu\", kernel_initializer='uniform'))\n",
    "    t_model.add(Dropout(0.4))\n",
    "    t_model.add(Dense(512, activation=\"relu\", kernel_initializer='uniform'))\n",
    "    t_model.add(Dropout(0.4))\n",
    "    t_model.add(Dense(256, activation=\"relu\", kernel_initializer='uniform'))\n",
    "    #                 kernel_regularizer=regularizers.l1(0.01), bias_regularizer=regularizers.l1(0.01)))\n",
    "    t_model.add(Dropout(0.5))\n",
    "    t_model.add(Dense(64, activation=\"relu\", kernel_initializer='uniform'))\n",
    "    #t_model.add(Dropout(0.4))\n",
    "    t_model.add(Dense(1, activation='relu'))\n",
    "    #adam = optimizers.Adam(lr=0.0001)\n",
    "    t_model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer='adam',\n",
    "        metrics=['mean_absolute_error'])\n",
    "    return(t_model)\n",
    "\n",
    "def lstm():\n",
    "    inputs = Input(shape=(Xs_train.shape[1], 1))\n",
    "    #inputs2 = Masking(mask_value=special_value, input_shape=(None,paddedX_train.shape[1],1))(inputs2)\n",
    "    lstm = LSTM(128, return_sequences=True)(inputs)\n",
    "    #lstm = Dropout(0.5)(lstm)\n",
    "    lstm = LSTM(64, return_sequences=True)(lstm)\n",
    "    #lstm = Dropout(0.3)(lstm)\n",
    "    #lstm = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.05))(lstm)\n",
    "    #lstm = Dropout(0.5)(lstm)\n",
    "\n",
    "    model_tot = Dense(64, activation='relu', kernel_initializer='glorot_uniform')(lstm)\n",
    "    model_tot = Dropout(0.1)(lstm)\n",
    "    model_tot = Flatten()(model_tot)\n",
    "    output = Dense(1, activation='relu')(model_tot)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    adam = optimizers.Adam(lr=0.00005)\n",
    "    model.compile(\n",
    "        loss='mean_squared_error', \n",
    "        optimizer=adam, \n",
    "        metrics=[\"mean_absolute_error\"])\n",
    "    return model\n",
    "\n",
    "def lstm_ann():\n",
    "    inputs1 = Input(shape=(Xt_train.shape[1],))\n",
    "    nn = Dense(64, activation='relu', kernel_initializer='glorot_uniform')(inputs1)\n",
    "    #nn = Dropout(0.4)(nn)\n",
    "    nn = Dense(256, activation='relu', kernel_initializer='glorot_uniform')(nn)\n",
    "    #nn = Dropout(0.4)(nn)\n",
    "    nn = Dense(512, activation='relu', kernel_initializer='glorot_uniform')(nn)\n",
    "    #nn = Dropout(0.4)(nn)\n",
    "    nn = Dense(256, activation='relu', kernel_initializer='glorot_uniform')(nn)\n",
    "    #nn = Dropout(0.4)(nn)\n",
    "    nn = Dense(64, activation='relu')(nn)\n",
    "    nn = Reshape((1, 64))(nn)\n",
    "\n",
    "    inputs2 = Input(shape=(Xs_train.shape[1], 1))\n",
    "    #inputs2 = Masking(mask_value=special_value, input_shape=(None,paddedX_train.shape[1],1))(inputs2)\n",
    "    lstm = LSTM(128, return_sequences=True)(inputs2)\n",
    "    #lstm = Dropout(0.2)(lstm)\n",
    "    lstm = LSTM(64, return_sequences=True)(lstm)\n",
    "    #lstm = Dropout(0.4)(lstm)\n",
    "    #lstm = LSTM(64, return_sequences=True)(lstm)\n",
    "    #lstm = Dropout(0.3)(lstm)\n",
    "\n",
    "    model_tot = concatenate([nn, lstm], axis=1)\n",
    "    model_tot = Dense(64, activation='relu', kernel_initializer='glorot_uniform')(model_tot)\n",
    "    #model_tot = Dropout(0.4)(model_tot)\n",
    "    model_tot = Flatten()(model_tot)\n",
    "    output = Dense(1, activation='relu')(model_tot)\n",
    "    \n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=output)\n",
    "    adam = optimizers.Adam(lr=0.00005)\n",
    "    model.compile(\n",
    "        loss='mean_squared_error', \n",
    "        optimizer=adam, \n",
    "        metrics=[\"mean_absolute_error\"])\n",
    "    return model\n",
    "\n",
    "def proposed():\n",
    "    inputs1 = Input(shape=(Xt_train.shape[1],))\n",
    "    inputs2 = Input(shape=(Xs_train.shape[1], 1))\n",
    "    \n",
    "    nn = Dense(128, activation='relu', kernel_initializer='glorot_uniform')(inputs1)\n",
    "    lstm = LSTM(units=128, return_sequences=True)(inputs2)\n",
    "    \n",
    "    add = Add()([nn, lstm])\n",
    "    nn = Dense(64, activation='relu', kernel_initializer='glorot_uniform')(add)\n",
    "    lstm = LSTM(units=64, return_sequences=True)(lstm)\n",
    "    \n",
    "    concat = concatenate([add, nn, lstm],axis=-1)\n",
    "    nn = Dense(32, activation='relu', kernel_initializer='glorot_uniform')(concat)\n",
    "    lstm = LSTM(units=32, return_sequences=True)(lstm)\n",
    "\n",
    "    concat = concatenate([add, nn, lstm], axis=-1)\n",
    "    nn = Dense(16, activation='relu', kernel_initializer='glorot_uniform')(add)\n",
    "    lstm = LSTM(units=16, return_sequences=True)(lstm)\n",
    "    \n",
    "    add = concatenate([add, nn, lstm], axis=-1)\n",
    "    nn = Dense(8, activation='relu', kernel_initializer='glorot_uniform')(add)\n",
    "    lstm = LSTM(units=8, return_sequences=True)(lstm)\n",
    "    \n",
    "    model_tot = concatenate([add, nn, lstm], axis=-1)\n",
    "\n",
    "    model_tot = Dense(units=4, activation='relu', kernel_regularizer=regularizers.l2(0.01), \n",
    "                      kernel_initializer='glorot_uniform')(model_tot)\n",
    "\n",
    "    model_tot = Flatten()(model_tot)\n",
    "    output = Dense(1, activation='relu')(model_tot)\n",
    "    \n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=output)\n",
    "    opt = optimizers.Adam(lr=1e-3)\n",
    "    model.compile(\n",
    "        loss='mean_squared_error', \n",
    "        optimizer=opt, \n",
    "        metrics=[\"mean_absolute_error\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0bba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Training the model(s) using k-fold cross validation. \n",
    "   The performance might be poor and require some parameter tuning stage.'''\n",
    "\n",
    "#model1 = ann()\n",
    "#model2 = lstm()\n",
    "#model3 = lstm_ann()\n",
    "model4 = proposed()\n",
    "\n",
    "#Here we only demonstrate it for the proposed model (model4)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "mae_cv = []\n",
    "mse_cv = []\n",
    "r2_cv = []\n",
    "\n",
    "for i in range(5):\n",
    "    result = next(kf.split(Xt_train), None)\n",
    "    t_train = Xt_train[result[0]]\n",
    "    s_train = Xs_train[result[0]]\n",
    "    p_train = y_train[result[0]]\n",
    "    t_valid = Xt_train[result[1]]\n",
    "    s_valid = Xs_train[result[1]]\n",
    "    p_valid = y_train[result[1]]\n",
    "    \n",
    "    mcp_save = ModelCheckpoint('model4/weight_cv.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "    history = model4.fit([t_train, s_train],\n",
    "                          p_train, \n",
    "                          validation_data=([t_valid, s_valid], p_valid), \n",
    "                          epochs=5,\n",
    "                          batch_size=64, \n",
    "                          verbose = 1,\n",
    "                          callbacks=[EarlyStopping(monitor='val_loss', patience=10, mode='min'), mcp_save])\n",
    "\n",
    "    pred = model4.predict([t_valid, s_valid])\n",
    "    pred = pred.reshape(p_valid.shape[0], 1)[:,-1]\n",
    "    \n",
    "    mae_cv.append(mean_absolute_error(p_valid, pred))\n",
    "    mse_cv.append(mean_squared_error(p_valid, pred)) \n",
    "    r2_cv.append(r2_score(p_valid, pred))\n",
    "    \n",
    "print(\"Average mse = %.2f (+/- %.2f)\" % (np.mean(mse_cv), np.std(mse_cv)))\n",
    "print(\"Best mse = %.2f \" % (np.min(mse_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e96099",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Evaluate the model(s) to the testing dataset'''\n",
    "\n",
    "# Create a new model instance\n",
    "model = proposed()\n",
    "\n",
    "# Restore the weights\n",
    "model.load_weights('model4/weight_cv.hdf5')\n",
    "\n",
    "# Evaluate the model\n",
    "pred_test = model.predict([Xt_test, Xs_test])\n",
    "pred_test = pred_test.reshape(y_test.shape[0], 1)[:,-1]\n",
    "    \n",
    "mae_test = mean_absolute_error(y_test, pred_test)\n",
    "mse_test = mean_squared_error(y_test, pred_test) \n",
    "r2_test = r2_score(y_test, pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
